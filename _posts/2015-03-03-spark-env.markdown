---
layout: post
title:  "Spark Env(Class which holds runtime objects)"
date:   2015-03-03 05:33:27 	
categories: Understanding Spark Execution Environment
---

A SparkEnv class holds various runtime objects for running a spark instance(either master or worker) on a machine.It includes akka actorsystem, blockmanager, serializer, mapoutputTracker. SparkEnv is a Global object which can be accessed by all the thread running into spark system.

After creating a SparkContext it can be easily accessed as SparkEnv.get.

<center>![SparkEnv image not found]({{ site.baseurl }}/images/SparkEnv.jpeg)</center>

SparkEnv manages python workers accros nodes to fork worker processes because it is exepensive to fork processes from java.

1- ExecuterId: The id of the process running on cluster, doing computations and storing data for our application.

2- ActorSystem: Akka ActorSystem in which BlockManagerMaster Actor and driver actor resides.

3- Serializer : for sending data to/from across nodes in serialized form.

4- ClosureSerializer : sending serealizer closure code across nodes for computations.

5- CacheManager : Spark class responsible for passing RDDs partition contents to the BlockManager and making sure a node doesn't load two copies of an RDD at once.

6- MapOutputTracker:  Class that keeps track of the location of the map output of a stage. This is abstract because different versions of MapOutputTracker (driver and executor) use different HashMap to store its metadata.

7- ShuffleManager : Pluggable interface for shuffle systems. A ShuffleManager is created in SparkEnv on the driver and on each executor, based on the spark.shuffle.manager setting. The driver registers shuffles with it, and executors (or tasks running locally in the driver) can ask to read and write data.
 
8- BroadCastManager :

9- BlockTransferService : This class is responsible for sending and retriving data blocks from nodes.

10- BlockManager : run on every node(driver or executers) which provides interfaces for sending blocks of data to and from various resources like memory, disks, off-heap both locally and remotely.

11- SecurityManager : This spark class implements security concerns in spark for transfering data.

12- HttpFileServer : A wrapper over jetty server for sending application jars and class across nodes.

13- SparkFilesDir : sparkFiles directory, used when downloading dependencies.  In local mode, this is a temporary directory; in distributed mode, this is the executor's current working directory.

14- MetricsSystem :

15- ShuffleMemoryManager : Allocates a pool of memory to task threads for use in shuffle operations. Each disk-spilling  collection (ExternalAppendOnlyMap or ExternalSorter) used by these tasks can acquire memory from this pool and release it as it spills data out. When a task ends, all its memory will be released by the Executor.This class tries to ensure that each thread gets a reasonable share of memory, instead of some
thread ramping up to a large amount first and then causing others to spill to disk repeatedly. If there are N threads, it ensures that each thread can acquire at least 1 / 2N of the memory before it has to spill, and at most 1 / N. Because N varies dynamically, we keep track of the set of active threads and redo the calculations of 1 / 2N and 1 / N in waiting threads whenever this set changes. This is all done by synchronizing access on "this" to mutate state and using  wait() and notifyAll() to signal changes.

16 -OutputCommitCoordinator : Authority that decides whether tasks can commit output to HDFS. Uses a "first committer wins" policy.OutputCommitCoordinator is instantiated in both the drivers and executors. On executors, it isconfigured with a reference to the driver's OutputCommitCoordinatorActor, so requests to commit output will be forwarded to the driver's OutputCommitCoordinator.

17 -SparkConf : Spark configuration object for holding the properties of spark apllication.


